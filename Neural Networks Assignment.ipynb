{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdc3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop on the Vowel Dataset\n",
    "# Inserting Required Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39538935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def loadCsv(file):\n",
    "    trainset = []\n",
    "    lines = csv.reader(open(file, 'r'))\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        for j in range(4):\n",
    "            # Print(\"DATA{}\".format(dataset[i]))\n",
    "            dataset[i][j] = float(dataset[i][j])\n",
    "            trainset.append(dataset[i])\n",
    "            return trainset\n",
    "        def minmax(dataset):\n",
    "            minmax = list()\n",
    "            stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "            return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bceaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0ad335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to float\n",
    "def column_to_float(dataset,column):\n",
    "    for row in dataset:\n",
    "        try:\n",
    "            row[column] = float(row[column])\n",
    "        except ValueError:\n",
    "                print(\"Error with row\",column,\":\",row[column])\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a46e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to integer\n",
    "def column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3d478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the min and max values for each column\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "            dataset_split.append(fold)\n",
    "            return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7825ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_met(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a84100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def run_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validaton_split(dataset, n_folds)\n",
    "    # for fold in folds:\n",
    "        # print(\"Fold{} \\n \\n\".format(fold))\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        # print(\"Test Fold {} \\n \\n\".format(fold))\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_met(actual, predicted)\n",
    "        cm = confusion_matrix(actual, predicted)\n",
    "        print('\\n'.join([''.join(['{:4}'.format(item) for item in row]) for row in cm]))\n",
    "        # Confusin matrix = np.matrix(cm)\n",
    "        FP = cm.sum(axis=0) - np.diag(cm)\n",
    "        FN = cm.sum(axis=1) - np.diag(cm)\n",
    "        TP = np.diag(cm)\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "        print('False Positivities\\n {}'.format(FP))\n",
    "        print('False Negativities\\n {}'.format(FN))\n",
    "        print('True Positivities\\n {}'.format(TP))\n",
    "        print('True Negativities\\n {}'.format(TN))\n",
    "        TPR = TP/(TP+FN)\n",
    "        print('Sensitivity \\n {}'.format(TPR))\n",
    "        TNR = TN/(TN+FP)\n",
    "        print('Specificity \\n {}'.format(TNR))\n",
    "        Precision = TP/(TP+FP)\n",
    "        print('Precision \\n {}.'.format(Precision))\n",
    "        Recall = TP/(TP+FN)\n",
    "        print('Recall \\n {}'.format(Recall))\n",
    "        Acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "        print('Accuracy \\n {}'.format(Acc))\n",
    "        Fscore = 2*(Precision*Recall)/(Precision+Recall)\n",
    "        print('FScore \\n {}'.format(Fscore))\n",
    "        k = cohen_kappa_score(actual, predicted)\n",
    "        print('Cohen Kappa \\n {}'.format(k))\n",
    "        scores.append(accuracy)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bbddc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Neuron Activation for an Input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "        return activation\n",
    "# Transfer Neuron Activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "848bd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propogate input to a Network Output\n",
    "def forward_propogate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "            inputs = new_inputs\n",
    "    return inputs\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "147dcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropogate error and store in neurons\n",
    "def backward_propogate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6638f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "            for neuron in network[i]:\n",
    "                for j in range(len(inputs)):\n",
    "                    temp = l_rate * neuron['delta'] * inputs[j] + mu * neuron['prev'][j]\n",
    "                    neuron['weights'][j] += temp\n",
    "                    # print(\"neuron weight {} \\n\".format(neuron['weights'][j]))\n",
    "                    neuron['prev'][j] = temp\n",
    "                    temp = l_rate * neuron['delta'] + mu * neuron['prev'][-1]\n",
    "                    neuron['weights'][-1] += temp\n",
    "                    neuron['prev'][-1] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80625f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propogate(network, row)\n",
    "            # print(network)\n",
    "            expected = [0 for i in range(n_outpus)]\n",
    "            expected[row[-1]] = 1\n",
    "            # print(\"expected roe {} \\n\".format(expected))\n",
    "            backward_propogate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)], 'prev':[0 for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)], 'prev':[0 for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_inputs + 1)], 'prev':[0 for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
    "    network.append(output_layer)\n",
    "    # print(network)\n",
    "    return network\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propogate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b880afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propogation Algorithm with Stochastic Gradient Descent\n",
    "def back_propogation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    # print(\"network {} \\n\".format(network))\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2289e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load and Prepare data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mloadCsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      7\u001b[0m     column_to_float(dataset, i)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mloadCsv\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadCsv\u001b[39m(file):\n\u001b[0;32m      3\u001b[0m     trainset \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m     lines \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(lines)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "# Test Backprop on Seeds Dataset\n",
    "seed(1)\n",
    "# Load and Prepare data\n",
    "file = \"data.csv\"\n",
    "dataset = loadCsv(file)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    column_to_float(dataset, i)\n",
    "    # Convert class column to integers\n",
    "    column_to_int(dataset, len(dataset[0])-1)\n",
    "    # Normalize input variables\n",
    "    minmax = minmax(dataset)\n",
    "    normalize(dataset, minmax)\n",
    "    # Evaluate algorithm\n",
    "    n_folds = 5\n",
    "    l_rate = 0.1\n",
    "    mu = 0.001\n",
    "    n_epoch = 1500\n",
    "    n_hidden = 4\n",
    "    scores = run_algorithm(dataset, back_propogation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "    # print('Scores: %s' % scores)\n",
    "    # print('Mean Accuracy: %.3f%%' %(sum(scores)/float(len(scores))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264f9de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
